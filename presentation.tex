\documentclass[compress]{beamer}
%--------------------------------------------------------------------------
% Common packages
%--------------------------------------------------------------------------
\usepackage[german]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{color}



% Erweiterte Tabellenfunktionen
\usepackage{tabularx,ragged2e}
\usepackage{booktabs}
% for the transition diagram
\usepackage{tikz}
\usetikzlibrary{arrows}
\setbeameroption{show notes}
% Listingserweiterung
\usepackage{listings}
\lstset{ %
  language=[LaTeX]TeX,
  basicstyle=\normalsize\ttfamily,
  keywordstyle=,
  numbers=left,
  numberstyle=\tiny\ttfamily,
  stepnumber=1,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  breaklines=true,
  frame=tb,
  framerule=0.5pt,
  tabsize=4,
  framexleftmargin=0.5em,
  framexrightmargin=0.5em,
  xleftmargin=0.5em,
  xrightmargin=0.5em
}


% ------------------------------------------------------------------------------
%  Tikz
% ------------------------------------------------------------------------------
\usetikzlibrary{arrows,positioning}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

%   NaturProzess




\newcommand{\captionslide}[1]{
  {\setbeamercolor{background canvas}{bg=hsrmWarmGreyLight}
    \begin{frame}[plain]
      \addtocounter{framenumber}{-1}
      \begin{center}
        \vfill\usebeamerfont{section title}\textcolor{black}{\MakeUppercase{#1}}\vfill
      \end{center}
    \end{frame}
  }
}



%--------------------------------------------------------------------------
% Load theme
%--------------------------------------------------------------------------
\usetheme[nosectionpages]{hsrm}


\usepackage{dtklogos} % must be loaded after theme
\usepackage{tikz}

\title{Statistical Modeling: The Two Cultures}
\date{}
\author{Christoph Molnar \\ Department of Statistics, \\ LMU Munich}
\subtitle{Based on Leo Breiman's paper}

\begin{document}
\maketitle
\addtocounter{framenumber}{-1}
\note{
  \textbf{Abstract:} \\
  This presentation compares two cultures of statistical modeling: the data modeling culture, which assumes a stochastic process that produced the data. This culture is associated with traditional statistics. The other culture is called algorithmic modeling culture, which can be reduced to optimization of a loss function with an algorithm. This culture is associated with Machine Learning. It is argued to use algorithmic modeling more often in statistics.
}

\begin{frame}{Outline}
  \begin{enumerate}
  \item Statistics: Data Modeling Culture
  \item Machine Learning: Algorithmic Modeling Culture
  \item Statistical Learning Principles
  \item Personal Experience
  \item Summary
  \end{enumerate}
  \let\thefootnote\relax\footnotetext{Content heavily based on: ``Statistical Modeling: The two cultures'' from Leo~Breiman \cite{breiman}}
\end{frame}

\note{
  This presentation is based on ``Statistical Modeling: The two cultures'' from Leo Breiman \cite{breiman}. \\
  The first segment introduces the data modeling culture and analogously the second segment explains the algorithmic modeling culture together with the presentation of three algorithms. The part about statistical learning principals presents aspects which help to compare both of cultures. Personal experiences in both cultures are addressed. The conclusion summarizes the message of the paper \cite{breiman}.
}


\section{Data Modeling}
\captionslide{Data Modeling Culture}


\begin{frame}[fragile]\frametitle{Work of a Data Analyst}
  \begin{itemize}
  \item   \textbf{Predict}
  \item   \textbf{Reveal associations}
  \item  Munge data, design experiments, visualize data,  \ldots
  \end{itemize}
\end{frame}

\note{
  The work of a data analyst is very diverse. This presentation focuses on the modeling of data, which can be reduced to two targets: Learn a model to predict the outcome for new covariates and get a better understanding about the relationship between covariates and outcome.
}


\begin{frame}[fragile]\frametitle{Simplified worldview}
  \begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {nature};
      \node[right=of natur] (x) {x};
      \node[left=of natur] (y) {y};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      ;
    \end{tikzpicture} \\
  \end{center}
\end{frame}

\note{
  In a strongly simplified world an arbitrary outcome y is produced by the nature given the covariates x. The knowledge about the natures true mechanisms range between entirely unknown and established (scientific) explanations of the mechanism. One example: Outcome y is the rent for apartments and covariates x are size, number of bathrooms and location.
}


\begin{frame}[fragile]\frametitle{Data Modeling Culture}
  \begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {Logistic Regression, \\Cox Model, \\GEE, \\ \ldots};
      \node[right=of natur] (x) {x};
      \node[left=of natur] (y) {y};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      ;
    \end{tikzpicture}
  \end{center}
  Find a stochastic model of the data-generating process: \\
  y = f(x, parameters, random error)
\end{frame}

\note{
  The direct modeling of the mechanism in the ``box'' is labeled \doublequoted{Data Modeling Culture} by Leo Breiman. In this culture a stochastic model for the data- generating process is assumed. A common formulation of these model is: y is a function of x with corresponding weights and a random error. For example: Given the covariates size, number of bathrooms and location, the rent of apartments is normal distributed.
}


\begin{frame}[fragile]\frametitle{Typical assumptions and restrictions}
  \begin{itemize}
  \item Specific stochastic model that generated the data
  \item Distribution of residuals
  \item Linearity (e.g. linear predictor)
  \item Manual specification of interactions
  \end{itemize}
\end{frame}


\begin{frame}[fragile]\frametitle{Problems}
  \begin{itemize}
  \item Conclusions about model, not about nature
  \item Assumptions often violated
  \item Often no model evaluation
  \item $\Rightarrow$ can lead to irrelevant theory and questionable statistical conclusions
  \item Focus not on prediction
  \item Data models fail in areas like image and speech recognition
  \end{itemize}
\end{frame}

\section{Algorithmic Modeling}
\captionslide{Algorithmic Modeling Culture}

\begin{frame}{Machine Learning}
  \begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {unknown};
      \node[right=of natur] (x) {x};
      \node[left=of natur] (y) {y};
      \node[below=of natur, label=below:algorithm] (algo) { \includegraphics*[width = 3cm]{./images/machine.png}};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      (x) edge[<->, bend left=30] node {} (algo)
      (y) edge[<->, bend right=30] node {} (algo)
      ;
    \end{tikzpicture}
  \end{center}
  Find a function $f(X)$ that minimizes the loss: $L(Y, f(X))$
\end{frame}

\note{
  In the \doublequoted{Algorithmic Modeling Culture}, the true mechanism is treated as unknown. It is not the target to find the true data-generating mechanism but to use an algorithm that imitates the mechanism as good as possible. Modeling is reduced to a mere problem of function optimization: Given the covariates x, outcome y and a loss function find a function f(x) which minimizes the loss for the prediction of the outcome. This culture is lived in the machine learning area. \\
  Summary: The data modeling culture tries to find the true data-generating mechanism, the algorithmic modeling culture tries to imitate the true mechanism as good as possible.}

\begin{frame}[fragile]\frametitle{Algorithm in Machine Learning}
  \begin{itemize}
  \item Boosting
  \item Support Vector Machines
  \item Artificial neural networks
  \item Random Forests
  \item Hidden Markov
  \item Bayes-Nets
  \item \ldots{} \footnote{Details and more algorithms in ``Elements of statistical learning''\cite{elements}}
  \end{itemize}

\end{frame}

\note{
  The algorithms used in machine learning are motivated differently. Three algorithms are presented in short.
}


\begin{frame}{Artificial neural networks}
  \includegraphics[width = 5cm]{./images/neurons.jpg}
  \hspace*{0.6cm}
  \includegraphics[width = 5cm]{./images/ANN.png}
  \footnote{\url{http://commons.wikimedia.org/wiki/File:Mouse_cingulate_cortex_neurons.jpg} \\ \url{http://commons.wikimedia.org/wiki/File:Neural_network.svg}}
\end{frame}

\note{
  Artificial neural networks are used in classification and regression. They are inspired by the brain, which consists of a network of brain cells (neurons). Mathematically artificial neural networks are a concatenation of weighted functions. An exemplary application is image processing. }


\begin{frame}{Support Vector Machines}
  \begin{center}
    \includegraphics[width = 7cm]{./images/SVM.JPG}
    \footnote{\url{http://commons.wikimedia.org/wiki/File:Svm_10_perceptron.JPG}}
  \end{center}
\end{frame}

\note{
  Support Vector Machines (SVM) were originally a classification method (regression is also possible). SVMs try to draw a border between to classes in the covariate space. The distance between the border and the class points is maximized. They use a mathematical trick to implicitly project the covariates in a space with higher dimensions (yes, it sounds a bit crazy) in order to achieve class separation. Text classification is an exemplary usage.
}

\begin{frame}{Random Forests}
  \begin{center}
    \includegraphics[width = 10cm]{./images/forest.png}
    \footnote{\url{http://openclipart.org/detail/175304/forest-by-z-175304}}
  \end{center}
\end{frame}

\note{
  Random Forests\texttrademark (invented by Leo Breiman) are used for regression and classification. A Random Forest consists of many decision trees. Two random mechanisms are used to train different trees on the data. Results from all trees are averaged for the prediction.
}

\section{Principles}
\captionslide{Statistical learning principles}

\begin{frame}{Rashomon Effect}
  \begin{center}
    (Often) Many different models describe a situation equally accurate.
  \end{center}
\end{frame}

\note{
  Rashomon is a Japanese movie in which four witnesses tell different versions of a crime. All versions account for the facts but they contradict each other. In terms of statistical learning this means, that often different models (e.g. same y but different covariates) can be equally accurate. Each model has a different interpretation which makes it difficult to find the true mechanism in the data modeling cultures. In the algorithmic modeling culture the Rashomon effect is exploited by aggregating over many models. Random forests use this effect by aggregating over many trees. It is also common to average the predictions of different algorithms. }

\begin{frame}{Dimensionality of the data}
  \begin{itemize}
  \item The higher the dimensionality of the data (\# covariates) the more difficult is the separation of signal and noise
  \item Common practice in data modeling: variable selection (by expert selection or data driven) and reduction of dimensionality (e.g. PCA)
  \item Common practice in algorithmic modeling: Engineering of new features (covariates) to increase predictive accuracy; algorithms robust for many covariates
  \end{itemize}
\end{frame}


\begin{frame}{Prediction vs. Interpretation}
  \begin{tikzpicture}
    \draw (0,0) -- (0,1.5) -- (9.9,0) -- (0,0);
    \draw  (0, 1.6) -- (10,1.6) -- (10, 0.1) -- (0, 1.6);
    \node at (3, 0.2) {Interpretability};
    \node at (7, 1.3) {Predictive accuracy};
    \node at (1.3, -0.5) {Tree};
    \node at (9, -0.5) {Random Forest};
    \node at (1.5, -1.2) {Logist. Regression};
    \node at (9, -1.2) {Neuronal networks};
    \node at (1.3, -1.9) {\ldots};
    \node at (9, -1.9) {\ldots};
  \end{tikzpicture}
\end{frame}

\note{
  There is a trade-off between interpretability and predictive accuracy: the models that are good in prediction are often complex and models that are easy to interpret are often bad predictors.
  See for example trees and Random Forests:
  A single decision tree is very intuitive and easy to read for non-professionals, but they are unstable and give weak predictions. A complex aggregation of decision trees (Random Forest) has an excellent prediction accuracy, but it is impossible to interpret the model structure. }

\begin{frame}{Goodness of model}
  \begin{itemize}
  \item Data modeling culture: Goodness of fit often based on model assumptions (e.g. AIC) and calculated on training data.
  \item Algorithmic modeling culture: Evaluation of predictive accuracy with an extra test set or cross validation.
  \end{itemize}
  How good is a statistical model if the predictive accuracy is weak? Is it legit to interpret parameters and p-values?
\end{frame}


\section{Experiences}
\captionslide{Personal Experiences}

\begin{frame}[fragile]\frametitle{Statistical Consulting}
  Stereotypical user ...
  \begin{itemize}
  \item are e.g. veterinarians, linguists, biologists, \ldots
  \item crave p-values
  \item want interpretability
  \item ignore model diagnosis
  \end{itemize}
\end{frame}

\note{
  From my experience in the statistical consulting unit of our university, most user want to test their scientific hypothesis with models. They want models which are easy to interpret regarding their questions. Thus it is more important to have a model that gives parameters associated with covariates and p-values than to have a model that predicts the data well.
}

\begin{frame}[fragile]\frametitle{kaggle}
  Algorithms of winners on kaggle,  a platform for prediction challenges:
  \begin{itemize}
    % http://blog.kaggle.com/2013/05/06/qa-with-job-salary-prediction-first-prize-winner-vlad-mnih/
  \item  Job Salary Prediction: \doublequoted{I used deep neural networks}
    % http://blog.kaggle.com/2012/12/19/1st-place-observing-dark-worlds/
  \item Observing Dark Worlds: \doublequoted{Bayesian analysis provided the winning recipe for solving this problem}
    % http://blog.kaggle.com/2012/01/03/the-perfect-storm-meet-the-winners-of-give-me-some-credit/
  \item Give Me Some Credit: \doublequoted{In the end we only used five supervised learning methods: a random forest of classification trees, a random forest of regression trees, a classification tree boosting algorithm, a regression tree boosting algorithm, and a neural network.}
  \end{itemize}
\end{frame}

\note{
  Kaggle (\url{http://www.kaggle.com}) is a platform for prediction challenges. Data are provided and the participant with the best prediction on a test set wins the challenge. To generate insights about the mechanisms in the data is secondary because the prediction is all that counts to win. That's why the algorithmic modeling culture is superior in this field.
}

\section{Conclusions}
\begin{frame}{Conclusion}
Data analysts should:
  \begin{itemize}
  \item Use predictive accuracy to evaluate models
  \item Seek the best model
  \item Add Machine Learning to their toolbox
  \end{itemize}
\end{frame}


\begin{frame}{Further literature}

  \begin{thebibliography}{10}
    \beamertemplatearticlebibitems
  \bibitem{breiman}
    L.~Breiman
    \newblock {\em Statistical modeling: The two cultures (with comments and a rejoinder by the author)}
    \newblock Institute of Mathematical Statistics, 2001.
    \beamertemplatebookbibitems
  \bibitem{elements}
    T.~Hastie, R.~Tibshirani and J.~Friedman
    \newblock {\em The elements of statistical learning}
    \newblock Springer New York, 2001
  \end{thebibliography}
\end{frame}


\begin{frame}[plain]
  \addtocounter{framenumber}{-1}
  \doublequoted{All models are wrong, but some are useful} (G. Box)
\end{frame}
\end{document}
